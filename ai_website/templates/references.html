{% extends "base.html" %}

{% block title %}References - Ryan Thompson{% endblock %}

{% block content %}
<section class="page-header">
    <div class="container">
        <h1 class="page-title">AI/ML References</h1>
        <p class="page-subtitle">Comprehensive guides, explanations, and code examples</p>
    </div>
</section>

<section class="references-content">
    <div class="container">
        <div class="references-grid">

            <!-- Machine Learning Fundamentals. -->
            <article class="reference-card">
                <div class="reference-header collapsed" onclick="toggleReferenceContent(this)">
                    <h2>Linear Regression</h2>
                    <span class="reference-category">Supervised Learning</span>
                </div>
                <div class="reference-body collapsed">
                    <p>
                        Linear regression is one of the fundamental algorithms in machine learning and statistics. It models
                        the relationship between a dependent variable (target) and one or more independent variables (features)
                        by fitting a linear equation to observed data. This technique is widely used for prediction, forecasting,
                        and understanding relationships between variables.
                    </p>

                    <h3>Mathematical Foundation</h3>
                    <p>
                        For simple linear regression with one feature, we want to find the best line that fits our data points.
                        The linear equation is:
                    </p>
                    <div class="math-equation">
                        <strong>≈∑ = mx + b</strong>
                    </div>
                    <p>Where:</p>
                    <ul>
                        <li><strong>≈∑</strong> = predicted output (dependent variable)</li>
                        <li><strong>x</strong> = input feature (independent variable)</li>
                        <li><strong>m</strong> = slope of the line</li>
                        <li><strong>b</strong> = y-intercept</li>
                    </ul>

                    <h3>Cost Function (Loss Function)</h3>
                    <p>
                        To find the best line, we need to minimize the difference between our predictions and actual values.
                        We use Mean Squared Error (MSE) as our cost function:
                    </p>
                    <div class="math-equation">
                        <strong>J(m,b) = (1/2n) Œ£·µ¢‚Çå‚ÇÅ‚Åø (≈∑·µ¢ - y·µ¢)¬≤</strong>
                    </div>
                    <p>Where:</p>
                    <ul>
                        <li><strong>J(m,b)</strong> = cost function</li>
                        <li><strong>n</strong> = number of training examples</li>
                        <li><strong>≈∑·µ¢</strong> = predicted value for example i</li>
                        <li><strong>y·µ¢</strong> = actual value for example i</li>
                    </ul>

                    <h3>Gradient Descent Optimization</h3>
                    <p>
                        To minimize the cost function, we use gradient descent. We calculate partial derivatives and
                        update our parameters iteratively:
                    </p>
                    <div class="math-equation">
                        <strong>‚àÇJ/‚àÇm = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø (≈∑·µ¢ - y·µ¢) √ó x·µ¢</strong><br>
                        <strong>‚àÇJ/‚àÇb = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø (≈∑·µ¢ - y·µ¢)</strong>
                    </div>
                    <p>Parameter updates:</p>
                    <div class="math-equation">
                        <strong>m := m - Œ± √ó ‚àÇJ/‚àÇm</strong><br>
                        <strong>b := b - Œ± √ó ‚àÇJ/‚àÇb</strong>
                    </div>
                    <p>Where <strong>Œ±</strong> (alpha) is the learning rate that controls the step size.</p>

                    <h3>Key Concepts</h3>
                    <ul>
                        <li><strong>Learning Rate (Œ±):</strong> Controls how big steps we take toward the minimum</li>
                        <li><strong>Convergence:</strong> When the algorithm finds the optimal parameters</li>
                        <li><strong>Feature Scaling:</strong> Normalizing inputs for better performance</li>
                        <li><strong>R-squared:</strong> Measure of how well the model explains variance in data</li>
                    </ul>

                    <h3>Worked Mathematical Examples</h3>
                    <p>Let's work through some concrete examples to see how the math works in practice:</p>

                    <h4>Example 1: Simple Dataset</h4>
                    <p>Consider predicting house prices based on size with this small dataset:</p>
                    <div class="math-equation">
                        <strong>Data Points:</strong><br>
                        (1000 sq ft, $150,000), (1500 sq ft, $225,000), (2000 sq ft, $300,000)
                    </div>

                    <h4>Step 1: Initialize Parameters</h4>
                    <p>Start with random values (or zeros):</p>
                    <div class="math-equation">
                        <strong>m = 0, b = 0</strong><br>
                        <strong>Œ± = 0.00001 (learning rate)</strong>
                    </div>

                    <h4>Step 2: Make Initial Predictions</h4>
                    <p>Using ≈∑ = mx + b with m=0, b=0:</p>
                    <div class="math-equation">
                        <strong>≈∑‚ÇÅ = 0√ó1000 + 0 = 0</strong><br>
                        <strong>≈∑‚ÇÇ = 0√ó1500 + 0 = 0</strong><br>
                        <strong>≈∑‚ÇÉ = 0√ó2000 + 0 = 0</strong>
                    </div>

                    <h4>Step 3: Calculate Cost (MSE)</h4>
                    <p>Using J = (1/2n) Œ£(≈∑·µ¢ - y·µ¢)¬≤ with n=3:</p>
                    <div class="math-equation">
                        <strong>J = (1/6)[(0-150000)¬≤ + (0-225000)¬≤ + (0-300000)¬≤]</strong><br>
                        <strong>J = (1/6)[22.5√ó10‚Åπ + 50.6√ó10‚Åπ + 90√ó10‚Åπ] = 27.18√ó10‚Åπ</strong>
                    </div>

                    <h4>Step 4: Calculate Gradients</h4>
                    <p>Calculate partial derivatives:</p>
                    <div class="math-equation">
                        <strong>‚àÇJ/‚àÇm = (1/3)[(0-150000)√ó1000 + (0-225000)√ó1500 + (0-300000)√ó2000]</strong><br>
                        <strong>‚àÇJ/‚àÇm = (1/3)[-150√ó10‚Å∂ - 337.5√ó10‚Å∂ - 600√ó10‚Å∂] = -362.5√ó10‚Å∂</strong><br><br>
                        <strong>‚àÇJ/‚àÇb = (1/3)[(0-150000) + (0-225000) + (0-300000)]</strong><br>
                        <strong>‚àÇJ/‚àÇb = (1/3)[-675000] = -225000</strong>
                    </div>

                    <h4>Step 5: Update Parameters</h4>
                    <p>Apply gradient descent updates:</p>
                    <div class="math-equation">
                        <strong>m = 0 - 0.00001√ó(-362.5√ó10‚Å∂) = 3625</strong><br>
                        <strong>b = 0 - 0.00001√ó(-225000) = 2.25</strong>
                    </div>

                    <h4>Step 6: New Predictions</h4>
                    <p>After one iteration with updated parameters:</p>
                    <div class="math-equation">
                        <strong>≈∑‚ÇÅ = 3625√ó1000 + 2.25 = 3,627,002.25</strong><br>
                        <strong>≈∑‚ÇÇ = 3625√ó1500 + 2.25 = 5,437,502.25</strong><br>
                        <strong>≈∑‚ÇÉ = 3625√ó2000 + 2.25 = 7,250,002.25</strong>
                    </div>

                    <p><em>Note: These predictions are way off! This shows why we need many iterations and proper learning rate tuning.</em></p>

                    <h4>Example 2: Convergence After Many Iterations</h4>
                    <p>After 1000 iterations with proper learning rate (Œ± = 0.0000001):</p>
                    <div class="math-equation">
                        <strong>m ‚âà 150 (slope: $150 per sq ft)</strong><br>
                        <strong>b ‚âà 0 (y-intercept close to zero)</strong><br>
                        <strong>Final equation: ≈∑ = 150x + 0</strong>
                    </div>

                    <p>Perfect predictions:</p>
                    <div class="math-equation">
                        <strong>≈∑‚ÇÅ = 150√ó1000 = $150,000 ‚úì</strong><br>
                        <strong>≈∑‚ÇÇ = 150√ó1500 = $225,000 ‚úì</strong><br>
                        <strong>≈∑‚ÇÉ = 150√ó2000 = $300,000 ‚úì</strong><br>
                        <strong>Final Cost J ‚âà 0</strong>
                    </div>

                    <h4>Example 3: R-squared Calculation</h4>
                    <p>For our final model, calculate how well it explains the data:</p>
                    <div class="math-equation">
                        <strong>»≥ = (150000 + 225000 + 300000)/3 = 225,000</strong><br><br>
                        <strong>SS_res = (150000-150000)¬≤ + (225000-225000)¬≤ + (300000-300000)¬≤ = 0</strong><br>
                        <strong>SS_tot = (150000-225000)¬≤ + (225000-225000)¬≤ + (300000-225000)¬≤</strong><br>
                        <strong>SS_tot = 75000¬≤ + 0¬≤ + 75000¬≤ = 11.25√ó10‚Åπ</strong><br><br>
                        <strong>R¬≤ = 1 - (0/11.25√ó10‚Åπ) = 1.0</strong>
                    </div>
                    <p>Perfect fit! R¬≤ = 1.0 means our model explains 100% of the variance.</p>

                    <h3>Visual Understanding</h3>
                    <p>Let's visualize how linear regression works with our house price example:</p>

                    <div class="visualization-container">
                        <h4>üìä Data Points and Fitted Line</h4>
                        <div class="graph-container">
                            <canvas id="dataFitCanvas" width="500" height="300"></canvas>
                        </div>

                        <h4>üìâ Cost Function Over Iterations</h4>
                        <div class="graph-container">
                            <canvas id="costCanvas" width="500" height="300"></canvas>
                            <div class="graph-controls">
                                <button onclick="showCostReduction()" class="graph-btn">Show Cost Reduction</button>
                                <button onclick="animateGradientDescent()" class="graph-btn">Animate Gradient Descent</button>
                            </div>
                        </div>

                        <h4>üéØ Interactive Parameter Explorer</h4>
                        <div class="interactive-demo">
                            <div class="parameter-controls">
                                <label>Slope (m): <span id="slopeValue">150</span></label>
                                <input type="range" id="slopeSlider" min="50" max="250" value="150" oninput="updateLine()">

                                <label>Intercept (b): <span id="interceptValue">0</span></label>
                                <input type="range" id="interceptSlider" min="-50000" max="50000" value="0" oninput="updateLine()">

                                <div class="cost-display">Cost (MSE): <span id="currentCost">0</span></div>
                            </div>
                            <canvas id="interactiveCanvas" width="500" height="300"></canvas>
                        </div>
                    </div>

                    <h3>Complete Implementation from Scratch</h3>
                    <p>
                        Below is a complete implementation of linear regression using gradient descent, built without
                        any machine learning libraries. This shows exactly how the mathematical concepts translate to code:
                    </p>
                    <pre class="code-block"><code><span class="keyword">import</span> <span class="builtin">numpy</span> <span class="keyword">as</span> <span class="builtin">np</span>
<span class="keyword">import</span> <span class="builtin">matplotlib.pyplot</span> <span class="keyword">as</span> <span class="builtin">plt</span>

<span class="keyword">class</span> <span class="class-name">LinearRegression</span><span class="punctuation">:</span>
    <span class="string">"""Linear Regression implemented from scratch using gradient descent."""</span>
    <span class="keyword">def</span> <span class="function">__init__</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">learning_rate</span><span class="operator">=</span><span class="number">0.01</span><span class="punctuation">,</span> <span class="variable">max_iterations</span><span class="operator">=</span><span class="number">1000</span><span class="punctuation">):</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">=</span> <span class="variable">learning_rate</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">max_iterations</span> <span class="operator">=</span> <span class="variable">max_iterations</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">slope</span> <span class="operator">=</span> <span class="constant">None</span>  <span class="comment"># m in y = mx + b</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">intercept</span> <span class="operator">=</span> <span class="constant">None</span>  <span class="comment"># b in y = mx + b</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">cost_history</span> <span class="operator">=</span> <span class="punctuation">[]</span>

    <span class="keyword">def</span> <span class="function">fit</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">):</span>
        <span class="string">"""Train the linear regression model."""</span>
        <span class="comment"># Initialize parameters</span>
        <span class="variable">n_samples</span><span class="punctuation">,</span> <span class="variable">n_features</span> <span class="operator">=</span> <span class="variable">X</span><span class="punctuation">.</span><span class="variable">shape</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">slope</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">zeros</span><span class="punctuation">(</span><span class="variable">n_features</span><span class="punctuation">)</span>  <span class="comment"># m parameter</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">intercept</span> <span class="operator">=</span> <span class="number">0</span>  <span class="comment"># b parameter</span>
        <span class="comment"># Gradient descent</span>
        <span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="builtin">range</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">max_iterations</span><span class="punctuation">):</span>
            <span class="comment"># Forward pass: make predictions using y = mx + b</span>
            <span class="variable">y_predicted</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">predict</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>
            <span class="comment"># Calculate cost (Mean Squared Error)</span>
            <span class="variable">cost</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">_calculate_cost</span><span class="punctuation">(</span><span class="variable">y</span><span class="punctuation">,</span> <span class="variable">y_predicted</span><span class="punctuation">)</span>
            <span class="variable">self</span><span class="punctuation">.</span><span class="variable">cost_history</span><span class="punctuation">.</span><span class="method">append</span><span class="punctuation">(</span><span class="variable">cost</span><span class="punctuation">)</span>
            <span class="comment"># Calculate gradients: ‚àÇJ/‚àÇm and ‚àÇJ/‚àÇb</span>
            <span class="variable">dm</span> <span class="operator">=</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span><span class="variable">n_samples</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">.</span><span class="variable">T</span><span class="punctuation">,</span> <span class="punctuation">(</span><span class="variable">y_predicted</span> <span class="operator">-</span> <span class="variable">y</span><span class="punctuation">))</span>
            <span class="variable">db</span> <span class="operator">=</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span><span class="variable">n_samples</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">(</span><span class="variable">y_predicted</span> <span class="operator">-</span> <span class="variable">y</span><span class="punctuation">)</span>
            <span class="comment"># Update parameters: m = m - Œ±*‚àÇJ/‚àÇm, b = b - Œ±*‚àÇJ/‚àÇb</span>
            <span class="variable">self</span><span class="punctuation">.</span><span class="variable">slope</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">dm</span>
            <span class="variable">self</span><span class="punctuation">.</span><span class="variable">intercept</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">db</span>

    <span class="keyword">def</span> <span class="function">predict</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">):</span>
        <span class="string">"""Make predictions using y = mx + b."""</span>
        <span class="keyword">return</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">,</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">slope</span><span class="punctuation">)</span> <span class="operator">+</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">intercept</span>

    <span class="keyword">def</span> <span class="function">_calculate_cost</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">y_true</span><span class="punctuation">,</span> <span class="variable">y_pred</span><span class="punctuation">):</span>
        <span class="string">"""Calculate Mean Squared Error."""</span>
        <span class="keyword">return</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">mean</span><span class="punctuation">((</span><span class="variable">y_true</span> <span class="operator">-</span> <span class="variable">y_pred</span><span class="punctuation">)</span> <span class="operator">**</span> <span class="number">2</span><span class="punctuation">)</span>

<span class="comment"># Example usage with real data</span>
<span class="keyword">if</span> <span class="variable">__name__</span> <span class="operator">==</span> <span class="string">"__main__"</span><span class="punctuation">:</span>
    <span class="comment"># Generate sample data: house sizes vs prices</span>
    <span class="builtin">np</span><span class="punctuation">.</span><span class="builtin">random</span><span class="punctuation">.</span><span class="method">seed</span><span class="punctuation">(</span><span class="number">42</span><span class="punctuation">)</span>
    <span class="variable">house_sizes</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="builtin">random</span><span class="punctuation">.</span><span class="method">uniform</span><span class="punctuation">(</span><span class="number">1000</span><span class="punctuation">,</span> <span class="number">3000</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">).</span><span class="method">reshape</span><span class="punctuation">(-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">)</span>
    <span class="comment"># Price = 150 * size + noise</span>
    <span class="variable">house_prices</span> <span class="operator">=</span> <span class="number">150</span> <span class="operator">*</span> <span class="variable">house_sizes</span><span class="punctuation">.</span><span class="method">flatten</span><span class="punctuation">()</span> <span class="operator">+</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="builtin">random</span><span class="punctuation">.</span><span class="method">normal</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">20000</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">)</span>
    <span class="comment"># Create and train model</span>
    <span class="variable">model</span> <span class="operator">=</span> <span class="class-name">LinearRegression</span><span class="punctuation">(</span><span class="variable">learning_rate</span><span class="operator">=</span><span class="number">0.0000001</span><span class="punctuation">,</span> <span class="variable">max_iterations</span><span class="operator">=</span><span class="number">1000</span><span class="punctuation">)</span>
    <span class="variable">model</span><span class="punctuation">.</span><span class="method">fit</span><span class="punctuation">(</span><span class="variable">house_sizes</span><span class="punctuation">,</span> <span class="variable">house_prices</span><span class="punctuation">)</span>
    <span class="comment"># Make predictions</span>
    <span class="variable">test_sizes</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">1500</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">2000</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">2500</span><span class="punctuation">]])</span>
    <span class="variable">predictions</span> <span class="operator">=</span> <span class="variable">model</span><span class="punctuation">.</span><span class="method">predict</span><span class="punctuation">(</span><span class="variable">test_sizes</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">"House Size -> Predicted Price"</span><span class="punctuation">)</span>
    <span class="keyword">for</span> <span class="variable">size</span><span class="punctuation">,</span> <span class="variable">price</span> <span class="keyword">in</span> <span class="builtin">zip</span><span class="punctuation">(</span><span class="variable">test_sizes</span><span class="punctuation">.</span><span class="method">flatten</span><span class="punctuation">(),</span> <span class="variable">predictions</span><span class="punctuation">):</span>
        <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"</span><span class="punctuation">{</span><span class="variable">size</span><span class="punctuation">}</span><span class="string"> sq ft -> $</span><span class="punctuation">{</span><span class="variable">price</span><span class="punctuation">:</span><span class="string">,.0f</span><span class="punctuation">}</span><span class="string">"</span><span class="punctuation">)</span>
    <span class="comment"># Calculate R-squared score</span>
    <span class="variable">y_pred_all</span> <span class="operator">=</span> <span class="variable">model</span><span class="punctuation">.</span><span class="method">predict</span><span class="punctuation">(</span><span class="variable">house_sizes</span><span class="punctuation">)</span>
    <span class="variable">ss_res</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">((</span><span class="variable">house_prices</span> <span class="operator">-</span> <span class="variable">y_pred_all</span><span class="punctuation">)</span> <span class="operator">**</span> <span class="number">2</span><span class="punctuation">)</span>
    <span class="variable">ss_tot</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">((</span><span class="variable">house_prices</span> <span class="operator">-</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">mean</span><span class="punctuation">(</span><span class="variable">house_prices</span><span class="punctuation">))</span> <span class="operator">**</span> <span class="number">2</span><span class="punctuation">)</span>
    <span class="variable">r_squared</span> <span class="operator">=</span> <span class="number">1</span> <span class="operator">-</span> <span class="punctuation">(</span><span class="variable">ss_res</span> <span class="operator">/</span> <span class="variable">ss_tot</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"R-squared: </span><span class="punctuation">{</span><span class="variable">r_squared</span><span class="punctuation">:</span><span class="string">.3f</span><span class="punctuation">}</span><span class="string">"</span><span class="punctuation">)</span>
    <span class="comment"># Output:</span>
    <span class="comment"># House Size -> Predicted Price</span>
    <span class="comment"># 1500 sq ft -> $225,180</span>
    <span class="comment"># 2000 sq ft -> $300,240</span>
    <span class="comment"># 2500 sq ft -> $375,300</span>
    <span class="comment"># R-squared: 0.926</span></code></pre>
                </div>
            </article>

            <!-- Logistic Regression. -->
            <article class="reference-card">
                <div class="reference-header collapsed" onclick="toggleReferenceContent(this)">
                    <h2>Logistic Regression</h2>
                    <span class="reference-category">Classification</span>
                </div>
                <div class="reference-body collapsed">
                    <p>
                        Logistic regression is a supervised learning algorithm used for binary classification. Instead of predicting a continuous value, it predicts the probability that an input belongs to the positive class, then applies a threshold (typically 0.5) to produce a class label.
                    </p>

                    <h3>Mathematical Foundation</h3>
                    <p>The model uses the sigmoid function to map real values to probabilities between 0 and 1.</p>
                    <div class="math-equation">
                        <strong>œÉ(z) = 1 / (1 + e‚Åª·∂ª)</strong><br>
                        <strong>h<sub>Œ∏</sub>(x) = œÉ(Œ∏·µÄx)</strong>
                    </div>
                    <p>Decision rule:</p>
                    <div class="math-equation">
                        <strong>≈∑ = 1</strong> if <strong>h<sub>Œ∏</sub>(x) ‚â• 0.5</strong>, else <strong>≈∑ = 0</strong>.<br>
                        Equivalent: <strong>Œ∏·µÄx ‚â• 0</strong> implies class 1, otherwise class 0.
                    </div>

                    <h3>Loss Function (Log Loss)</h3>
                    <p>We optimize the negative log-likelihood (cross-entropy) over m examples:</p>
                    <div class="math-equation">
                        <strong>L(y, h) = -[y¬∑log(h) + (1-y)¬∑log(1-h)]</strong><br>
                        <strong>J(Œ∏) = (1/m) Œ£·µ¢ L(y·µ¢, h<sub>Œ∏</sub>(x·µ¢))</strong>
                    </div>

                    <h3>Gradient and Updates</h3>
                    <p>The gradient of the loss with respect to parameters is:</p>
                    <div class="math-equation">
                        <strong>‚àÇJ/‚àÇŒ∏ = (1/m) X·µÄ(h - y)</strong>
                    </div>
                    <p>Gradient descent update:</p>
                    <div class="math-equation">
                        <strong>Œ∏ := Œ∏ - Œ± ¬∑ (1/m) X·µÄ(h - y)</strong>
                    </div>

                    <h3>Regularization</h3>
                    <p>To reduce overfitting, add L2 regularization (Œª ‚â• 0):</p>
                    <div class="math-equation">
                        <strong>J<sub>reg</sub>(Œ∏) = J(Œ∏) + (Œª/2m) Œ£<sub>j=1</sub><sup>n</sup> Œ∏<sub>j</sub>¬≤</strong>
                    </div>
                    <p>Update for j ‚â• 1:</p>
                    <div class="math-equation">
                        <strong>Œ∏<sub>j</sub> := Œ∏<sub>j</sub> - Œ±¬∑[(1/m) Œ£(h - y)¬∑x<sub>j</sub> + (Œª/m) Œ∏<sub>j</sub>]</strong><br>
                        Bias term Œ∏<sub>0</sub> is not regularized.
                    </div>

                    <h3>Worked Example</h3>
                    <p>Binary classification with a single feature x and labels y ‚àà {0,1}:</p>
                    <div class="math-equation">
                        <strong>Data:</strong> (x, y) = (0.5, 0), (1.0, 0), (2.0, 1), (3.0, 1).<br>
                        Initialize Œ∏ = [Œ∏‚ÇÄ, Œ∏‚ÇÅ] = [0, 0], Œ± = 0.1.
                    </div>
                    <p>Compute h = œÉ(Œ∏‚ÇÄ + Œ∏‚ÇÅx) for each point, accumulate gradient (h - y)x, and update Œ∏ repeatedly until convergence.</p>

                    <h3>Interactive Decision Boundary</h3>
                    <div class="interactive-demo">
                        <div class="parameter-controls">
                            <label>Bias (Œ∏‚ÇÄ): <span id="theta0Value">-4.0</span></label>
                            <input type="range" id="theta0Slider" min="-5" max="5" step="0.1" value="-4" oninput="updateLogistic()">

                            <label>Weight x‚ÇÅ (Œ∏‚ÇÅ): <span id="theta1Value">2.0</span></label>
                            <input type="range" id="theta1Slider" min="-5" max="5" step="0.1" value="2" oninput="updateLogistic()">

                            <label>Weight x‚ÇÇ (Œ∏‚ÇÇ): <span id="theta2Value">2.0</span></label>
                            <input type="range" id="theta2Slider" min="-5" max="5" step="0.1" value="2" oninput="updateLogistic()">

                            <label>Threshold (t): <span id="threshValue">0.50</span></label>
                            <input type="range" id="threshSlider" min="0.1" max="0.9" step="0.05" value="0.5" oninput="updateLogistic()">
                        </div>
                        <canvas id="logisticCanvas" width="500" height="300"></canvas>
                    </div>

                    <h3>Implementation from Scratch</h3>
                    <p>A compact NumPy implementation of binary logistic regression using gradient descent:</p>
                    <pre class="code-block"><code><span class="keyword">import</span> <span class="builtin">numpy</span> <span class="keyword">as</span> <span class="builtin">np</span>

<span class="keyword">class</span> <span class="class-name">LogisticRegressionGD</span><span class="punctuation">:</span>
    <span class="string">"""Binary Logistic Regression trained via gradient descent."""</span>

    <span class="keyword">def</span> <span class="function">__init__</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">learning_rate</span><span class="operator">=</span><span class="number">0.1</span><span class="punctuation">,</span> <span class="variable">max_iterations</span><span class="operator">=</span><span class="number">1000</span><span class="punctuation">,</span> <span class="variable">l2</span><span class="operator">=</span><span class="number">0.0</span><span class="punctuation">):</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">=</span> <span class="variable">learning_rate</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">max_iterations</span> <span class="operator">=</span> <span class="variable">max_iterations</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">l2</span> <span class="operator">=</span> <span class="variable">l2</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">theta</span> <span class="operator">=</span> <span class="constant">None</span>

    <span class="keyword">def</span> <span class="function">_sigmoid</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">z</span><span class="punctuation">):</span>
        <span class="string">"""Numerically stable sigmoid."""</span>
        <span class="variable">z</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">clip</span><span class="punctuation">(</span><span class="variable">z</span><span class="punctuation">,</span> <span class="operator">-</span><span class="number">250</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span>
        <span class="keyword">return</span> <span class="number">1</span> <span class="operator">/</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">exp</span><span class="punctuation">(-</span><span class="variable">z</span><span class="punctuation">))</span>

    <span class="keyword">def</span> <span class="function">fit</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">):</span>
        <span class="string">"""Train on features X (m√ón) and targets y (m,)."""</span>
        <span class="variable">m</span><span class="punctuation">,</span> <span class="variable">n</span> <span class="operator">=</span> <span class="variable">X</span><span class="punctuation">.</span><span class="variable">shape</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">theta</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">zeros</span><span class="punctuation">(</span><span class="variable">n</span> <span class="operator">+</span> <span class="number">1</span><span class="punctuation">)</span>  <span class="comment"># Include bias.</span>
        <span class="variable">Xb</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">c_</span><span class="punctuation">[</span><span class="builtin">np</span><span class="punctuation">.</span><span class="method">ones</span><span class="punctuation">(</span><span class="variable">m</span><span class="punctuation">),</span> <span class="variable">X</span><span class="punctuation">]</span>
        <span class="keyword">for</span> <span class="variable">_</span> <span class="keyword">in</span> <span class="builtin">range</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">max_iterations</span><span class="punctuation">)</span>:
            <span class="variable">z</span> <span class="operator">=</span> <span class="variable">Xb</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">theta</span><span class="punctuation">)</span>
            <span class="variable">h</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">_sigmoid</span><span class="punctuation">(</span><span class="variable">z</span><span class="punctuation">)</span>
            <span class="variable">gradient</span> <span class="operator">=</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span><span class="variable">m</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="variable">Xb</span><span class="punctuation">.</span><span class="variable">T</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">h</span> <span class="operator">-</span> <span class="variable">y</span><span class="punctuation">)</span>
            <span class="comment"># L2 regularization on weights only (skip bias at index 0).</span>
            <span class="keyword">if</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">l2</span> <span class="operator">></span> <span class="number">0</span><span class="punctuation">:</span>
                <span class="variable">reg</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">theta</span><span class="punctuation">.</span><span class="method">copy</span><span class="punctuation">()</span>
                <span class="variable">reg</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]</span> <span class="operator">=</span> <span class="number">0</span>
                <span class="variable">gradient</span> <span class="operator">+=</span> <span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">l2</span> <span class="operator">/</span> <span class="variable">m</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="variable">reg</span>
            <span class="variable">self</span><span class="punctuation">.</span><span class="variable">theta</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">gradient</span>

    <span class="keyword">def</span> <span class="function">predict_proba</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">):</span>
        <span class="string">"""Return probabilities P(y=1|x)."""</span>
        <span class="variable">m</span> <span class="operator">=</span> <span class="variable">X</span><span class="punctuation">.</span><span class="variable">shape</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]</span>
        <span class="variable">Xb</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">c_</span><span class="punctuation">[</span><span class="builtin">np</span><span class="punctuation">.</span><span class="method">ones</span><span class="punctuation">(</span><span class="variable">m</span><span class="punctuation">),</span> <span class="variable">X</span><span class="punctuation">]</span>
        <span class="keyword">return</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">_sigmoid</span><span class="punctuation">(</span><span class="variable">Xb</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">theta</span><span class="punctuation">))</span>

    <span class="keyword">def</span> <span class="function">predict</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">,</span> <span class="variable">threshold</span><span class="operator">=</span><span class="number">0.5</span><span class="punctuation">):</span>
        <span class="string">"""Return class labels using a threshold (default 0.5)."""</span>
        <span class="variable">proba</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">predict_proba</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>
        <span class="keyword">return</span> <span class="punctuation">(</span><span class="variable">proba</span> <span class="operator">>=</span> <span class="variable">threshold</span><span class="punctuation">)</span><span class="punctuation">.</span><span class="method">astype</span><span class="punctuation">(</span><span class="builtin">int</span><span class="punctuation">)</span>

<span class="comment"># Example usage.</span>
<span class="keyword">if</span> <span class="variable">__name__</span> <span class="operator">==</span> <span class="string">"__main__"</span><span class="punctuation">:</span>
    <span class="comment"># Simple separable dataset.</span>
    <span class="variable">X</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">0.5</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">1.0</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">2.0</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">3.0</span><span class="punctuation">]])</span>
    <span class="variable">y</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">])</span>
    <span class="variable">clf</span> <span class="operator">=</span> <span class="class-name">LogisticRegressionGD</span><span class="punctuation">(</span><span class="variable">learning_rate</span><span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> <span class="variable">max_iterations</span><span class="operator">=</span><span class="number">2000</span><span class="punctuation">)</span>
    <span class="variable">clf</span><span class="punctuation">.</span><span class="method">fit</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">)</span>
    <span class="variable">proba</span> <span class="operator">=</span> <span class="variable">clf</span><span class="punctuation">.</span><span class="method">predict_proba</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>
    <span class="variable">pred</span> <span class="operator">=</span> <span class="variable">clf</span><span class="punctuation">.</span><span class="method">predict</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">"Probabilities:"</span><span class="punctuation">,</span> <span class="variable">proba</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">"Predictions:"</span><span class="punctuation">,</span> <span class="variable">pred</span><span class="punctuation">)</span></code></pre>
                </div>
            </article>

            <!-- Neural Networks. -->
            <article class="reference-card">
                <div class="reference-header collapsed" onclick="toggleReferenceContent(this)">
                    <h2>Neural Networks</h2>
                    <span class="reference-category">Deep Learning</span>
                </div>
                <div class="reference-body collapsed">
                    <p>
                        Neural networks are computing systems inspired by biological neural networks. They consist of
                        interconnected nodes (neurons) that process information through weighted connections. These networks
                        can learn complex patterns and relationships in data, making them powerful tools for tasks like
                        image recognition, natural language processing, and prediction.
                    </p>

                    <h3>Network Architecture</h3>
                    <p>
                        A neural network consists of layers of interconnected neurons. The basic structure includes:
                    </p>
                    <div class="math-equation">
                        <strong>Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer</strong>
                    </div>
                    <p>Key components:</p>
                    <ul>
                        <li><strong>Input Layer:</strong> Receives the input features (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)</li>
                        <li><strong>Hidden Layers:</strong> Process information through weighted connections</li>
                        <li><strong>Output Layer:</strong> Produces the final prediction or classification</li>
                        <li><strong>Weights (W):</strong> Parameters that determine connection strength</li>
                        <li><strong>Biases (b):</strong> Threshold values for neuron activation</li>
                    </ul>

                    <h3>Forward Propagation</h3>
                    <p>
                        Information flows forward through the network from input to output. For each neuron:
                    </p>
                    <div class="math-equation">
                        <strong>z = W¬∑x + b</strong><br>
                        <strong>a = œÉ(z)</strong>
                    </div>
                    <p>Where:</p>
                    <ul>
                        <li><strong>z</strong> = weighted sum of inputs plus bias</li>
                        <li><strong>W</strong> = weight matrix</li>
                        <li><strong>x</strong> = input vector</li>
                        <li><strong>b</strong> = bias vector</li>
                        <li><strong>œÉ</strong> = activation function</li>
                        <li><strong>a</strong> = activated output</li>
                    </ul>

                    <h3>Activation Functions</h3>
                    <p>
                        Activation functions introduce non-linearity, allowing networks to learn complex patterns:
                    </p>

                    <h4>Common Activation Functions</h4>
                    <div class="math-equation">
                        <strong>Sigmoid: œÉ(z) = 1 / (1 + e‚Åª·∂ª)</strong><br>
                        <strong>ReLU: f(z) = max(0, z)</strong><br>
                        <strong>Tanh: f(z) = (e·∂ª - e‚Åª·∂ª) / (e·∂ª + e‚Åª·∂ª)</strong>
                    </div>
                    <ul>
                        <li><strong>Sigmoid:</strong> Outputs between 0 and 1, good for binary classification</li>
                        <li><strong>ReLU:</strong> Most popular, computationally efficient, helps with vanishing gradients</li>
                        <li><strong>Tanh:</strong> Outputs between -1 and 1, zero-centered</li>
                    </ul>

                    <h3>Backpropagation Algorithm</h3>
                    <p>
                        Backpropagation is the learning algorithm that adjusts weights based on prediction errors:
                    </p>

                    <h4>Step 1: Calculate Loss</h4>
                    <div class="math-equation">
                        <strong>Loss = (1/2) √ó (≈∑ - y)¬≤</strong> (for regression)<br>
                        <strong>Loss = -y√ólog(≈∑) - (1-y)√ólog(1-≈∑)</strong> (for classification)
                    </div>

                    <h4>Step 2: Calculate Gradients (Chain Rule)</h4>
                    <div class="math-equation">
                        <strong>‚àÇLoss/‚àÇW = ‚àÇLoss/‚àÇa √ó ‚àÇa/‚àÇz √ó ‚àÇz/‚àÇW</strong><br>
                        <strong>‚àÇLoss/‚àÇb = ‚àÇLoss/‚àÇa √ó ‚àÇa/‚àÇz √ó ‚àÇz/‚àÇb</strong>
                    </div>

                    <h4>Step 3: Update Parameters</h4>
                    <div class="math-equation">
                        <strong>W := W - Œ± √ó ‚àÇLoss/‚àÇW</strong><br>
                        <strong>b := b - Œ± √ó ‚àÇLoss/‚àÇb</strong>
                    </div>

                    <h3>Worked Example: XOR Problem</h3>
                    <p>A classic example showing why we need hidden layers to solve non-linear problems:</p>

                    <h4>Problem Setup</h4>
                    <div class="math-equation">
                        <strong>XOR Truth Table:</strong><br>
                        (0,0) ‚Üí 0 | (0,1) ‚Üí 1 | (1,0) ‚Üí 1 | (1,1) ‚Üí 0
                    </div>

                    <h4>Network Architecture</h4>
                    <p>Simple 2-2-1 network (2 inputs, 2 hidden neurons, 1 output):</p>
                    <div class="math-equation">
                        <strong>Hidden Layer:</strong><br>
                        h‚ÇÅ = œÉ(w‚ÇÅ‚ÇÅx‚ÇÅ + w‚ÇÅ‚ÇÇx‚ÇÇ + b‚ÇÅ)<br>
                        h‚ÇÇ = œÉ(w‚ÇÇ‚ÇÅx‚ÇÅ + w‚ÇÇ‚ÇÇx‚ÇÇ + b‚ÇÇ)<br><br>
                        <strong>Output Layer:</strong><br>
                        ≈∑ = œÉ(w‚ÇÉ‚ÇÅh‚ÇÅ + w‚ÇÉ‚ÇÇh‚ÇÇ + b‚ÇÉ)
                    </div>

                    <h4>Example Forward Pass</h4>
                    <p>For input (1,0) with learned weights:</p>
                    <div class="math-equation">
                        <strong>w‚ÇÅ‚ÇÅ=6, w‚ÇÅ‚ÇÇ=6, b‚ÇÅ=-3 ‚Üí h‚ÇÅ = œÉ(6√ó1 + 6√ó0 - 3) = œÉ(3) ‚âà 0.95</strong><br>
                        <strong>w‚ÇÇ‚ÇÅ=-6, w‚ÇÇ‚ÇÇ=-6, b‚ÇÇ=9 ‚Üí h‚ÇÇ = œÉ(-6√ó1 - 6√ó0 + 9) = œÉ(3) ‚âà 0.95</strong><br>
                        <strong>w‚ÇÉ‚ÇÅ=7, w‚ÇÉ‚ÇÇ=-7, b‚ÇÉ=-3 ‚Üí ≈∑ = œÉ(7√ó0.95 - 7√ó0.95 - 3) ‚âà œÉ(-3) ‚âà 0.05</strong>
                    </div>
                    <p>But XOR(1,0) should be 1, so we adjust weights through backpropagation!</p>

                    <h3>Key Concepts</h3>
                    <ul>
                        <li><strong>Universal Approximation:</strong> Neural networks can approximate any continuous function</li>
                        <li><strong>Depth vs Width:</strong> Deeper networks can learn more complex features</li>
                        <li><strong>Overfitting:</strong> When the model memorizes training data but fails on new data</li>
                        <li><strong>Regularization:</strong> Techniques like dropout to prevent overfitting</li>
                        <li><strong>Gradient Descent Variants:</strong> SGD, Adam, RMSprop for optimization</li>
                    </ul>

                    <h3>Applications</h3>
                    <ul>
                        <li><strong>Image Classification:</strong> CNNs for recognizing objects in photos</li>
                        <li><strong>Natural Language Processing:</strong> RNNs and Transformers for text analysis</li>
                        <li><strong>Speech Recognition:</strong> Converting audio to text</li>
                        <li><strong>Game Playing:</strong> AI systems that master complex games</li>
                        <li><strong>Medical Diagnosis:</strong> Analyzing medical images and data</li>
                    </ul>

                    <h3>Simple Neural Network Implementation</h3>
                    <p>
                        Here's a basic implementation of a neural network for binary classification:
                    </p>
                    <pre class="code-block"><code><span class="keyword">import</span> <span class="builtin">numpy</span> <span class="keyword">as</span> <span class="builtin">np</span>

<span class="keyword">class</span> <span class="class-name">SimpleNeuralNetwork</span><span class="punctuation">:</span>
    <span class="string">"""A simple 2-layer neural network for binary classification."""</span>

    <span class="keyword">def</span> <span class="function">__init__</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">input_size</span><span class="punctuation">,</span> <span class="variable">hidden_size</span><span class="punctuation">,</span> <span class="variable">learning_rate</span><span class="operator">=</span><span class="number">0.1</span><span class="punctuation">):</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">=</span> <span class="variable">learning_rate</span>

        <span class="comment"># Initialize weights randomly</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W1</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="builtin">random</span><span class="punctuation">.</span><span class="method">randn</span><span class="punctuation">(</span><span class="variable">input_size</span><span class="punctuation">,</span> <span class="variable">hidden_size</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="number">0.5</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">b1</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">zeros</span><span class="punctuation">((</span><span class="number">1</span><span class="punctuation">,</span> <span class="variable">hidden_size</span><span class="punctuation">))</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W2</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="builtin">random</span><span class="punctuation">.</span><span class="method">randn</span><span class="punctuation">(</span><span class="variable">hidden_size</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="number">0.5</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">b2</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">zeros</span><span class="punctuation">((</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">))</span>

    <span class="keyword">def</span> <span class="function">sigmoid</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">z</span><span class="punctuation">):</span>
        <span class="string">"""Sigmoid activation function."""</span>
        <span class="keyword">return</span> <span class="number">1</span> <span class="operator">/</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">exp</span><span class="punctuation">(-</span><span class="builtin">np</span><span class="punctuation">.</span><span class="method">clip</span><span class="punctuation">(</span><span class="variable">z</span><span class="punctuation">,</span> <span class="operator">-</span><span class="number">250</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)))</span>

    <span class="keyword">def</span> <span class="function">sigmoid_derivative</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">z</span><span class="punctuation">):</span>
        <span class="string">"""Derivative of sigmoid function."""</span>
        <span class="keyword">return</span> <span class="variable">z</span> <span class="operator">*</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> <span class="variable">z</span><span class="punctuation">)</span>

    <span class="keyword">def</span> <span class="function">forward</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">):</span>
        <span class="string">"""Forward propagation through the network."""</span>
        <span class="comment"># Hidden layer</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">z1</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">,</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W1</span><span class="punctuation">)</span> <span class="operator">+</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">b1</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">a1</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">sigmoid</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">z1</span><span class="punctuation">)</span>

        <span class="comment"># Output layer</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">z2</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">a1</span><span class="punctuation">,</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W2</span><span class="punctuation">)</span> <span class="operator">+</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">b2</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">a2</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">sigmoid</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">z2</span><span class="punctuation">)</span>

        <span class="keyword">return</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">a2</span>

    <span class="keyword">def</span> <span class="function">backward</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">,</span> <span class="variable">output</span><span class="punctuation">):</span>
        <span class="string">"""Backpropagation to calculate gradients."""</span>
        <span class="variable">m</span> <span class="operator">=</span> <span class="variable">X</span><span class="punctuation">.</span><span class="variable">shape</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]</span>

        <span class="comment"># Output layer gradients</span>
        <span class="variable">dZ2</span> <span class="operator">=</span> <span class="variable">output</span> <span class="operator">-</span> <span class="variable">y</span>
        <span class="variable">dW2</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">a1</span><span class="punctuation">.</span><span class="variable">T</span><span class="punctuation">,</span> <span class="variable">dZ2</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="variable">m</span>
        <span class="variable">db2</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">(</span><span class="variable">dZ2</span><span class="punctuation">,</span> <span class="variable">axis</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> <span class="variable">keepdims</span><span class="operator">=</span><span class="constant">True</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="variable">m</span>

        <span class="comment"># Hidden layer gradients</span>
        <span class="variable">dA1</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">dZ2</span><span class="punctuation">,</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W2</span><span class="punctuation">.</span><span class="variable">T</span><span class="punctuation">)</span>
        <span class="variable">dZ1</span> <span class="operator">=</span> <span class="variable">dA1</span> <span class="operator">*</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">sigmoid_derivative</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">a1</span><span class="punctuation">)</span>
        <span class="variable">dW1</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">dot</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">.</span><span class="variable">T</span><span class="punctuation">,</span> <span class="variable">dZ1</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="variable">m</span>
        <span class="variable">db1</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">(</span><span class="variable">dZ1</span><span class="punctuation">,</span> <span class="variable">axis</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> <span class="variable">keepdims</span><span class="operator">=</span><span class="constant">True</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="variable">m</span>

        <span class="comment"># Update parameters</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W2</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">dW2</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">b2</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">db2</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">W1</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">dW1</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">b1</span> <span class="operator">-=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">learning_rate</span> <span class="operator">*</span> <span class="variable">db1</span>

    <span class="keyword">def</span> <span class="function">train</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">,</span> <span class="variable">epochs</span><span class="operator">=</span><span class="number">1000</span><span class="punctuation">):</span>
        <span class="string">"""Train the neural network."""</span>
        <span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="builtin">range</span><span class="punctuation">(</span><span class="variable">epochs</span><span class="punctuation">):</span>
            <span class="comment"># Forward propagation</span>
            <span class="variable">output</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">forward</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>

            <span class="comment"># Backpropagation</span>
            <span class="variable">self</span><span class="punctuation">.</span><span class="method">backward</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">,</span> <span class="variable">output</span><span class="punctuation">)</span>

            <span class="comment"># Print loss every 100 epochs</span>
            <span class="keyword">if</span> <span class="variable">i</span> <span class="operator">%</span> <span class="number">100</span> <span class="operator">==</span> <span class="number">0</span><span class="punctuation">:</span>
                <span class="variable">loss</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">mean</span><span class="punctuation">(</span><span class="builtin">np</span><span class="punctuation">.</span><span class="method">square</span><span class="punctuation">(</span><span class="variable">output</span> <span class="operator">-</span> <span class="variable">y</span><span class="punctuation">))</span>
                <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"Epoch </span><span class="punctuation">{</span><span class="variable">i</span><span class="punctuation">}</span><span class="string">, Loss: </span><span class="punctuation">{</span><span class="variable">loss</span><span class="punctuation">:</span><span class="string">.4f</span><span class="punctuation">}</span><span class="string">"</span><span class="punctuation">)</span>

    <span class="keyword">def</span> <span class="function">predict</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">):</span>
        <span class="string">"""Make predictions on new data."""</span>
        <span class="keyword">return</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">forward</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>

<span class="comment"># Example: Solving XOR problem</span>
<span class="keyword">if</span> <span class="variable">__name__</span> <span class="operator">==</span> <span class="string">"__main__"</span><span class="punctuation">:</span>
    <span class="comment"># XOR dataset</span>
    <span class="variable">X</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">],</span>
                        <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">],</span>
                        <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">],</span>
                        <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]])</span>
    <span class="variable">y</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">0</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]])</span>

    <span class="comment"># Create and train network</span>
    <span class="variable">nn</span> <span class="operator">=</span> <span class="class-name">SimpleNeuralNetwork</span><span class="punctuation">(</span><span class="variable">input_size</span><span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> <span class="variable">hidden_size</span><span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> <span class="variable">learning_rate</span><span class="operator">=</span><span class="number">1.0</span><span class="punctuation">)</span>
    <span class="variable">nn</span><span class="punctuation">.</span><span class="method">train</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">,</span> <span class="variable">epochs</span><span class="operator">=</span><span class="number">2000</span><span class="punctuation">)</span>

    <span class="comment"># Test predictions</span>
    <span class="variable">predictions</span> <span class="operator">=</span> <span class="variable">nn</span><span class="punctuation">.</span><span class="method">predict</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">"\\nPredictions:"</span><span class="punctuation">)</span>
    <span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="builtin">range</span><span class="punctuation">(</span><span class="builtin">len</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)):</span>
        <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"Input: </span><span class="punctuation">{</span><span class="variable">X</span><span class="punctuation">[</span><span class="variable">i</span><span class="punctuation">]}</span><span class="string">, Target: </span><span class="punctuation">{</span><span class="variable">y</span><span class="punctuation">[</span><span class="variable">i</span><span class="punctuation">][</span><span class="number">0</span><span class="punctuation">]}</span><span class="string">, Prediction: </span><span class="punctuation">{</span><span class="variable">predictions</span><span class="punctuation">[</span><span class="variable">i</span><span class="punctuation">][</span><span class="number">0</span><span class="punctuation">]:</span><span class="string">.3f</span><span class="punctuation">}</span><span class="string">"</span><span class="punctuation">)</span>
    <span class="comment"># Expected output:</span>
    <span class="comment"># Input: [0 0], Target: 0, Prediction: ~0.05</span>
    <span class="comment"># Input: [0 1], Target: 1, Prediction: ~0.95</span>
    <span class="comment"># Input: [1 0], Target: 1, Prediction: ~0.95</span>
    <span class="comment"># Input: [1 1], Target: 0, Prediction: ~0.05</span></code></pre>
                </div>
            </article>

            <!-- K-Nearest Neighbors -->
            <article class="reference-card">
                <div class="reference-header collapsed" onclick="toggleReferenceContent(this)">
                    <h2>K-Nearest Neighbors (KNN)</h2>
                    <span class="reference-category">Instance-Based Learning</span>
                </div>
                <div class="reference-body collapsed">
                    <p>
                        K-Nearest Neighbors (KNN) is one of the simplest and most intuitive machine learning algorithms.
                        It's a non-parametric method that makes predictions based on the 'k' closest training examples
                        in the feature space. KNN is called "lazy learning" because it doesn't build an explicit model
                        during training - instead, it stores all training data and makes predictions at query time.
                    </p>

                    <h3>Core Algorithm</h3>
                    <p>
                        The KNN algorithm follows a simple 3-step process:
                    </p>
                    <div class="math-equation">
                        <strong>1. Calculate distances to all training points</strong><br>
                        <strong>2. Find the k nearest neighbors</strong><br>
                        <strong>3. Make prediction based on neighbors</strong>
                    </div>
                    <ul>
                        <li><strong>Classification:</strong> Majority vote among k neighbors</li>
                        <li><strong>Regression:</strong> Average of k neighbor values</li>
                        <li><strong>Distance Metric:</strong> Usually Euclidean, Manhattan, or Minkowski</li>
                    </ul>

                    <h3>Distance Metrics</h3>
                    <p>
                        The choice of distance metric significantly affects KNN performance:
                    </p>

                    <h4>Euclidean Distance</h4>
                    <div class="math-equation">
                        <strong>d(p,q) = ‚àö[(p‚ÇÅ-q‚ÇÅ)¬≤ + (p‚ÇÇ-q‚ÇÇ)¬≤ + ... + (p‚Çô-q‚Çô)¬≤]</strong>
                    </div>
                    <p>Most common choice, works well for continuous features</p>

                    <h4>Manhattan Distance</h4>
                    <div class="math-equation">
                        <strong>d(p,q) = |p‚ÇÅ-q‚ÇÅ| + |p‚ÇÇ-q‚ÇÇ| + ... + |p‚Çô-q‚Çô|</strong>
                    </div>
                    <p>Good for high-dimensional data, less sensitive to outliers</p>

                    <h4>Minkowski Distance (Generalized)</h4>
                    <div class="math-equation">
                        <strong>d(p,q) = (Œ£·µ¢|p·µ¢-q·µ¢| ≥)^(1/r)</strong><br>
                        <strong>r=1: Manhattan | r=2: Euclidean</strong>
                    </div>

                    <h3>Choosing the Right K</h3>
                    <p>
                        The value of k is crucial for good performance:
                    </p>
                    <ul>
                        <li><strong>k=1:</strong> Sensitive to noise, overfitting risk</li>
                        <li><strong>k too large:</strong> Over-smoothing, underfitting risk</li>
                        <li><strong>Odd k:</strong> Preferred for binary classification (avoids ties)</li>
                        <li><strong>Rule of thumb:</strong> k = ‚àön where n is number of training samples</li>
                        <li><strong>Cross-validation:</strong> Best method to find optimal k</li>
                    </ul>

                    <h3>Worked Example: Iris Classification</h3>
                    <p>Let's classify iris flowers using KNN with k=3:</p>

                    <h4>Dataset Sample</h4>
                    <div class="math-equation">
                        <strong>Training Data (Sepal Length, Sepal Width, Species):</strong><br>
                        (5.1, 3.5, Setosa) | (7.0, 3.2, Versicolor) | (6.3, 3.3, Virginica)<br>
                        (4.9, 3.0, Setosa) | (6.4, 3.2, Versicolor) | (5.8, 2.7, Virginica)
                    </div>

                    <h4>New Sample to Classify</h4>
                    <div class="math-equation">
                        <strong>Query Point: (5.5, 3.1, ?)</strong>
                    </div>

                    <h4>Step 1: Calculate Distances</h4>
                    <p>Using Euclidean distance:</p>
                    <div class="math-equation">
                        <strong>d‚ÇÅ = ‚àö[(5.5-5.1)¬≤ + (3.1-3.5)¬≤] = ‚àö[0.16 + 0.16] = 0.57</strong><br>
                        <strong>d‚ÇÇ = ‚àö[(5.5-7.0)¬≤ + (3.1-3.2)¬≤] = ‚àö[2.25 + 0.01] = 1.50</strong><br>
                        <strong>d‚ÇÉ = ‚àö[(5.5-6.3)¬≤ + (3.1-3.3)¬≤] = ‚àö[0.64 + 0.04] = 0.82</strong><br>
                        <strong>d‚ÇÑ = ‚àö[(5.5-4.9)¬≤ + (3.1-3.0)¬≤] = ‚àö[0.36 + 0.01] = 0.61</strong><br>
                        <strong>d‚ÇÖ = ‚àö[(5.5-6.4)¬≤ + (3.1-3.2)¬≤] = ‚àö[0.81 + 0.01] = 0.91</strong><br>
                        <strong>d‚ÇÜ = ‚àö[(5.5-5.8)¬≤ + (3.1-2.7)¬≤] = ‚àö[0.09 + 0.16] = 0.50</strong>
                    </div>

                    <h4>Step 2: Find 3 Nearest Neighbors</h4>
                    <div class="math-equation">
                        <strong>Nearest neighbors (sorted by distance):</strong><br>
                        <strong>1st: d‚ÇÜ = 0.50 ‚Üí Virginica</strong><br>
                        <strong>2nd: d‚ÇÅ = 0.57 ‚Üí Setosa</strong><br>
                        <strong>3rd: d‚ÇÑ = 0.61 ‚Üí Setosa</strong>
                    </div>

                    <h4>Step 3: Majority Vote</h4>
                    <div class="math-equation">
                        <strong>Votes: Setosa(2), Virginica(1)</strong><br>
                        <strong>Prediction: Setosa</strong>
                    </div>

                    <h3>KNN for Regression Example</h3>
                    <p>Predicting house prices using k=3:</p>

                    <h4>Query: House with 1800 sq ft</h4>
                    <div class="math-equation">
                        <strong>Nearest neighbors:</strong><br>
                        <strong>1750 sq ft ‚Üí $280,000 (distance: 50)</strong><br>
                        <strong>1820 sq ft ‚Üí $295,000 (distance: 20)</strong><br>
                        <strong>1780 sq ft ‚Üí $285,000 (distance: 20)</strong><br><br>
                        <strong>Prediction = (280,000 + 295,000 + 285,000) √∑ 3 = $286,667</strong>
                    </div>

                    <h3>Weighted KNN</h3>
                    <p>
                        Give closer neighbors more influence on the prediction:
                    </p>
                    <div class="math-equation">
                        <strong>Weight = 1 / distance</strong><br>
                        <strong>Prediction = Œ£(weight √ó value) / Œ£(weights)</strong>
                    </div>
                    <p>Using the house price example:</p>
                    <div class="math-equation">
                        <strong>w‚ÇÅ = 1/50 = 0.02, w‚ÇÇ = 1/20 = 0.05, w‚ÇÉ = 1/20 = 0.05</strong><br>
                        <strong>Weighted Price = (0.02√ó280,000 + 0.05√ó295,000 + 0.05√ó285,000) / 0.12</strong><br>
                        <strong>= (5,600 + 14,750 + 14,250) / 0.12 = $287,500</strong>
                    </div>

                    <h3>Advantages and Disadvantages</h3>

                    <h4>Advantages</h4>
                    <ul>
                        <li><strong>Simple and Intuitive:</strong> Easy to understand and implement</li>
                        <li><strong>No Training Period:</strong> Lazy learning approach</li>
                        <li><strong>Versatile:</strong> Works for both classification and regression</li>
                        <li><strong>Non-parametric:</strong> Makes no assumptions about data distribution</li>
                        <li><strong>Adapts to New Data:</strong> Simply add new points to training set</li>
                    </ul>

                    <h4>Disadvantages</h4>
                    <ul>
                        <li><strong>Computationally Expensive:</strong> O(n) time complexity for each prediction</li>
                        <li><strong>Storage Requirements:</strong> Must store entire training dataset</li>
                        <li><strong>Curse of Dimensionality:</strong> Performance degrades in high dimensions</li>
                        <li><strong>Sensitive to Irrelevant Features:</strong> All features treated equally</li>
                        <li><strong>Sensitive to Scale:</strong> Requires feature normalization</li>
                    </ul>

                    <h3>Optimization Techniques</h3>
                    <ul>
                        <li><strong>Feature Scaling:</strong> Normalize features to same scale</li>
                        <li><strong>Dimensionality Reduction:</strong> Use PCA to reduce feature space</li>
                        <li><strong>Distance Weighting:</strong> Give closer neighbors more influence</li>
                        <li><strong>Data Structures:</strong> Use KD-trees or Ball trees for faster search</li>
                        <li><strong>Feature Selection:</strong> Remove irrelevant or noisy features</li>
                    </ul>

                    <h3>Complete KNN Implementation</h3>
                    <p>
                        Here's a full implementation of KNN for both classification and regression:
                    </p>
                    <pre class="code-block"><code><span class="keyword">import</span> <span class="builtin">numpy</span> <span class="keyword">as</span> <span class="builtin">np</span>
<span class="keyword">from</span> <span class="builtin">collections</span> <span class="keyword">import</span> <span class="builtin">Counter</span>
<span class="keyword">import</span> <span class="builtin">matplotlib.pyplot</span> <span class="keyword">as</span> <span class="builtin">plt</span>

<span class="keyword">class</span> <span class="class-name">KNearestNeighbors</span><span class="punctuation">:</span>
    <span class="string">"""K-Nearest Neighbors implementation for classification and regression."""</span>

    <span class="keyword">def</span> <span class="function">__init__</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">k</span><span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> <span class="variable">distance_metric</span><span class="operator">=</span><span class="string">'euclidean'</span><span class="punctuation">,</span> <span class="variable">weighted</span><span class="operator">=</span><span class="constant">False</span><span class="punctuation">):</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">k</span> <span class="operator">=</span> <span class="variable">k</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">distance_metric</span> <span class="operator">=</span> <span class="variable">distance_metric</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">weighted</span> <span class="operator">=</span> <span class="variable">weighted</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">X_train</span> <span class="operator">=</span> <span class="constant">None</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">y_train</span> <span class="operator">=</span> <span class="constant">None</span>

    <span class="keyword">def</span> <span class="function">euclidean_distance</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">p1</span><span class="punctuation">,</span> <span class="variable">p2</span><span class="punctuation">):</span>
        <span class="string">"""Calculate Euclidean distance between two points."""</span>
        <span class="keyword">return</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sqrt</span><span class="punctuation">(</span><span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">((</span><span class="variable">p1</span> <span class="operator">-</span> <span class="variable">p2</span><span class="punctuation">)</span> <span class="operator">**</span> <span class="number">2</span><span class="punctuation">))</span>

    <span class="keyword">def</span> <span class="function">manhattan_distance</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">p1</span><span class="punctuation">,</span> <span class="variable">p2</span><span class="punctuation">):</span>
        <span class="string">"""Calculate Manhattan distance between two points."""</span>
        <span class="keyword">return</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">sum</span><span class="punctuation">(</span><span class="builtin">np</span><span class="punctuation">.</span><span class="method">abs</span><span class="punctuation">(</span><span class="variable">p1</span> <span class="operator">-</span> <span class="variable">p2</span><span class="punctuation">))</span>

    <span class="keyword">def</span> <span class="function">calculate_distance</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">p1</span><span class="punctuation">,</span> <span class="variable">p2</span><span class="punctuation">):</span>
        <span class="string">"""Calculate distance based on selected metric."""</span>
        <span class="keyword">if</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">distance_metric</span> <span class="operator">==</span> <span class="string">'euclidean'</span><span class="punctuation">:</span>
            <span class="keyword">return</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">euclidean_distance</span><span class="punctuation">(</span><span class="variable">p1</span><span class="punctuation">,</span> <span class="variable">p2</span><span class="punctuation">)</span>
        <span class="keyword">elif</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">distance_metric</span> <span class="operator">==</span> <span class="string">'manhattan'</span><span class="punctuation">:</span>
            <span class="keyword">return</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">manhattan_distance</span><span class="punctuation">(</span><span class="variable">p1</span><span class="punctuation">,</span> <span class="variable">p2</span><span class="punctuation">)</span>
        <span class="keyword">else</span><span class="punctuation">:</span>
            <span class="keyword">raise</span> <span class="builtin">ValueError</span><span class="punctuation">(</span><span class="string">f"Unsupported distance metric: </span><span class="punctuation">{</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">distance_metric</span><span class="punctuation">}</span><span class="string">"</span><span class="punctuation">)</span>

    <span class="keyword">def</span> <span class="function">fit</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">,</span> <span class="variable">y</span><span class="punctuation">):</span>
        <span class="string">"""Store training data (lazy learning)."""</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">X_train</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">(</span><span class="variable">X</span><span class="punctuation">)</span>
        <span class="variable">self</span><span class="punctuation">.</span><span class="variable">y_train</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">(</span><span class="variable">y</span><span class="punctuation">)</span>

    <span class="keyword">def</span> <span class="function">get_neighbors</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">x_query</span><span class="punctuation">):</span>
        <span class="string">"""Find k nearest neighbors to query point."""</span>
        <span class="variable">distances</span> <span class="operator">=</span> <span class="punctuation">[]</span>
        <span class="keyword">for</span> <span class="variable">i</span><span class="punctuation">,</span> <span class="variable">x_train</span> <span class="keyword">in</span> <span class="builtin">enumerate</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">X_train</span><span class="punctuation">):</span>
            <span class="variable">dist</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">calculate_distance</span><span class="punctuation">(</span><span class="variable">x_query</span><span class="punctuation">,</span> <span class="variable">x_train</span><span class="punctuation">)</span>
            <span class="variable">distances</span><span class="punctuation">.</span><span class="method">append</span><span class="punctuation">((</span><span class="variable">dist</span><span class="punctuation">,</span> <span class="variable">i</span><span class="punctuation">))</span>
        <span class="comment"># Sort by distance and get k nearest</span>
        <span class="variable">distances</span><span class="punctuation">.</span><span class="method">sort</span><span class="punctuation">(</span><span class="variable">key</span><span class="operator">=</span><span class="keyword">lambda</span> <span class="variable">x</span><span class="punctuation">:</span> <span class="variable">x</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">])</span>
        <span class="variable">k_nearest</span> <span class="operator">=</span> <span class="variable">distances</span><span class="punctuation">[:</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">k</span><span class="punctuation">]</span>
        <span class="keyword">return</span> <span class="variable">k_nearest</span>

    <span class="keyword">def</span> <span class="function">predict_classification</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">):</span>
        <span class="string">"""Make classification predictions."""</span>
        <span class="variable">predictions</span> <span class="operator">=</span> <span class="punctuation">[]</span>
        <span class="keyword">for</span> <span class="variable">x_query</span> <span class="keyword">in</span> <span class="variable">X</span><span class="punctuation">:</span>
            <span class="variable">neighbors</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">get_neighbors</span><span class="punctuation">(</span><span class="variable">x_query</span><span class="punctuation">)</span>
            <span class="keyword">if</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">weighted</span><span class="punctuation">:</span>
                <span class="comment"># Weighted voting</span>
                <span class="variable">weighted_votes</span> <span class="operator">=</span> <span class="punctuation">{}</span>
                <span class="keyword">for</span> <span class="variable">dist</span><span class="punctuation">,</span> <span class="variable">idx</span> <span class="keyword">in</span> <span class="variable">neighbors</span><span class="punctuation">:</span>
                    <span class="variable">label</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">y_train</span><span class="punctuation">[</span><span class="variable">idx</span><span class="punctuation">]</span>
                    <span class="variable">weight</span> <span class="operator">=</span> <span class="number">1</span> <span class="operator">/</span> <span class="punctuation">(</span><span class="variable">dist</span> <span class="operator">+</span> <span class="number">1e-5</span><span class="punctuation">)</span>  <span class="comment"># Add small value to avoid division by zero</span>
                    <span class="variable">weighted_votes</span><span class="punctuation">[</span><span class="variable">label</span><span class="punctuation">]</span> <span class="operator">=</span> <span class="variable">weighted_votes</span><span class="punctuation">.</span><span class="method">get</span><span class="punctuation">(</span><span class="variable">label</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">)</span> <span class="operator">+</span> <span class="variable">weight</span>
                <span class="variable">prediction</span> <span class="operator">=</span> <span class="builtin">max</span><span class="punctuation">(</span><span class="variable">weighted_votes</span><span class="punctuation">,</span> <span class="variable">key</span><span class="operator">=</span><span class="variable">weighted_votes</span><span class="punctuation">.</span><span class="variable">get</span><span class="punctuation">)</span>
            <span class="keyword">else</span><span class="punctuation">:</span>
                <span class="comment"># Simple majority voting</span>
                <span class="variable">neighbor_labels</span> <span class="operator">=</span> <span class="punctuation">[</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">y_train</span><span class="punctuation">[</span><span class="variable">idx</span><span class="punctuation">]</span> <span class="keyword">for</span> <span class="variable">_</span><span class="punctuation">,</span> <span class="variable">idx</span> <span class="keyword">in</span> <span class="variable">neighbors</span><span class="punctuation">]</span>
                <span class="variable">prediction</span> <span class="operator">=</span> <span class="class-name">Counter</span><span class="punctuation">(</span><span class="variable">neighbor_labels</span><span class="punctuation">).</span><span class="method">most_common</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">)[</span><span class="number">0</span><span class="punctuation">][</span><span class="number">0</span><span class="punctuation">]</span>
            <span class="variable">predictions</span><span class="punctuation">.</span><span class="method">append</span><span class="punctuation">(</span><span class="variable">prediction</span><span class="punctuation">)</span>
        <span class="keyword">return</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">(</span><span class="variable">predictions</span><span class="punctuation">)</span>

    <span class="keyword">def</span> <span class="function">predict_regression</span><span class="punctuation">(</span><span class="variable">self</span><span class="punctuation">,</span> <span class="variable">X</span><span class="punctuation">):</span>
        <span class="string">"""Make regression predictions."""</span>
        <span class="variable">predictions</span> <span class="operator">=</span> <span class="punctuation">[]</span>
        <span class="keyword">for</span> <span class="variable">x_query</span> <span class="keyword">in</span> <span class="variable">X</span><span class="punctuation">:</span>
            <span class="variable">neighbors</span> <span class="operator">=</span> <span class="variable">self</span><span class="punctuation">.</span><span class="method">get_neighbors</span><span class="punctuation">(</span><span class="variable">x_query</span><span class="punctuation">)</span>
            <span class="keyword">if</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">weighted</span><span class="punctuation">:</span>
                <span class="comment"># Weighted average</span>
                <span class="variable">total_weight</span> <span class="operator">=</span> <span class="number">0</span>
                <span class="variable">weighted_sum</span> <span class="operator">=</span> <span class="number">0</span>
                <span class="keyword">for</span> <span class="variable">dist</span><span class="punctuation">,</span> <span class="variable">idx</span> <span class="keyword">in</span> <span class="variable">neighbors</span><span class="punctuation">:</span>
                    <span class="variable">weight</span> <span class="operator">=</span> <span class="number">1</span> <span class="operator">/</span> <span class="punctuation">(</span><span class="variable">dist</span> <span class="operator">+</span> <span class="number">1e-5</span><span class="punctuation">)</span>
                    <span class="variable">weighted_sum</span> <span class="operator">+=</span> <span class="variable">weight</span> <span class="operator">*</span> <span class="variable">self</span><span class="punctuation">.</span><span class="variable">y_train</span><span class="punctuation">[</span><span class="variable">idx</span><span class="punctuation">]</span>
                    <span class="variable">total_weight</span> <span class="operator">+=</span> <span class="variable">weight</span>
                <span class="variable">prediction</span> <span class="operator">=</span> <span class="variable">weighted_sum</span> <span class="operator">/</span> <span class="variable">total_weight</span>
            <span class="keyword">else</span><span class="punctuation">:</span>
                <span class="comment"># Simple average</span>
                <span class="variable">neighbor_values</span> <span class="operator">=</span> <span class="punctuation">[</span><span class="variable">self</span><span class="punctuation">.</span><span class="variable">y_train</span><span class="punctuation">[</span><span class="variable">idx</span><span class="punctuation">]</span> <span class="keyword">for</span> <span class="variable">_</span><span class="punctuation">,</span> <span class="variable">idx</span> <span class="keyword">in</span> <span class="variable">neighbors</span><span class="punctuation">]</span>
                <span class="variable">prediction</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">mean</span><span class="punctuation">(</span><span class="variable">neighbor_values</span><span class="punctuation">)</span>
            <span class="variable">predictions</span><span class="punctuation">.</span><span class="method">append</span><span class="punctuation">(</span><span class="variable">prediction</span><span class="punctuation">)</span>
        <span class="keyword">return</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">(</span><span class="variable">predictions</span><span class="punctuation">)</span>

<span class="comment"># Example usage</span>
<span class="keyword">if</span> <span class="variable">__name__</span> <span class="operator">==</span> <span class="string">"__main__"</span><span class="punctuation">:</span>
    <span class="comment"># Classification Example: Iris dataset</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">"=== KNN Classification Example ==="</span><span class="punctuation">)</span>

    <span class="comment"># Sample iris data (sepal_length, sepal_width)</span>
    <span class="variable">X_iris</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([</span>
        <span class="punctuation">[</span><span class="number">5.1</span><span class="punctuation">,</span> <span class="number">3.5</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">4.9</span><span class="punctuation">,</span> <span class="number">3.0</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">7.0</span><span class="punctuation">,</span> <span class="number">3.2</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">6.4</span><span class="punctuation">,</span> <span class="number">3.2</span><span class="punctuation">],</span>
        <span class="punctuation">[</span><span class="number">6.3</span><span class="punctuation">,</span> <span class="number">3.3</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">5.8</span><span class="punctuation">,</span> <span class="number">2.7</span><span class="punctuation">]</span>
    <span class="punctuation">])</span>
    <span class="variable">y_iris</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([</span><span class="string">'Setosa'</span><span class="punctuation">,</span> <span class="string">'Setosa'</span><span class="punctuation">,</span> <span class="string">'Versicolor'</span><span class="punctuation">,</span> <span class="string">'Versicolor'</span><span class="punctuation">,</span> <span class="string">'Virginica'</span><span class="punctuation">,</span> <span class="string">'Virginica'</span><span class="punctuation">])</span>
    <span class="comment"># Train KNN classifier</span>
    <span class="variable">knn_clf</span> <span class="operator">=</span> <span class="class-name">KNearestNeighbors</span><span class="punctuation">(</span><span class="variable">k</span><span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> <span class="variable">weighted</span><span class="operator">=</span><span class="constant">True</span><span class="punctuation">)</span>
    <span class="variable">knn_clf</span><span class="punctuation">.</span><span class="method">fit</span><span class="punctuation">(</span><span class="variable">X_iris</span><span class="punctuation">,</span> <span class="variable">y_iris</span><span class="punctuation">)</span>

    <span class="comment"># Test prediction</span>
    <span class="variable">test_point</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">5.5</span><span class="punctuation">,</span> <span class="number">3.1</span><span class="punctuation">]])</span>
    <span class="variable">prediction</span> <span class="operator">=</span> <span class="variable">knn_clf</span><span class="punctuation">.</span><span class="method">predict_classification</span><span class="punctuation">(</span><span class="variable">test_point</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"Test point: </span><span class="punctuation">{</span><span class="variable">test_point</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]}</span><span class="string">"</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"Predicted class: </span><span class="punctuation">{</span><span class="variable">prediction</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]}</span><span class="string">"</span><span class="punctuation">)</span>

    <span class="comment"># Regression Example: House prices</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">"\\n=== KNN Regression Example ==="</span><span class="punctuation">)</span>
    <span class="comment"># Sample house data (square feet)</span>
    <span class="variable">X_houses</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">1750</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">1820</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">1780</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">2100</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="number">1650</span><span class="punctuation">]])</span>
    <span class="variable">y_houses</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([</span><span class="number">280000</span><span class="punctuation">,</span> <span class="number">295000</span><span class="punctuation">,</span> <span class="number">285000</span><span class="punctuation">,</span> <span class="number">340000</span><span class="punctuation">,</span> <span class="number">260000</span><span class="punctuation">])</span>
    <span class="comment"># Train KNN regressor</span>
    <span class="variable">knn_reg</span> <span class="operator">=</span> <span class="class-name">KNearestNeighbors</span><span class="punctuation">(</span><span class="variable">k</span><span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> <span class="variable">weighted</span><span class="operator">=</span><span class="constant">True</span><span class="punctuation">)</span>
    <span class="variable">knn_reg</span><span class="punctuation">.</span><span class="method">fit</span><span class="punctuation">(</span><span class="variable">X_houses</span><span class="punctuation">,</span> <span class="variable">y_houses</span><span class="punctuation">)</span>
    <span class="comment"># Test prediction</span>
    <span class="variable">test_house</span> <span class="operator">=</span> <span class="builtin">np</span><span class="punctuation">.</span><span class="method">array</span><span class="punctuation">([[</span><span class="number">1800</span><span class="punctuation">]])</span>
    <span class="variable">price_prediction</span> <span class="operator">=</span> <span class="variable">knn_reg</span><span class="punctuation">.</span><span class="method">predict_regression</span><span class="punctuation">(</span><span class="variable">test_house</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"House size: </span><span class="punctuation">{</span><span class="variable">test_house</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">][</span><span class="number">0</span><span class="punctuation">]}</span><span class="string"> sq ft"</span><span class="punctuation">)</span>
    <span class="builtin">print</span><span class="punctuation">(</span><span class="string">f"Predicted price: $</span><span class="punctuation">{</span><span class="variable">price_prediction</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]:</span><span class="string">,.0f</span><span class="punctuation">}</span><span class="string">"</span><span class="punctuation">)</span>
    <span class="comment"># Output:</span>
    <span class="comment"># Test point: [5.5 3.1]</span>
    <span class="comment"># Predicted class: Setosa</span>
    <span class="comment"># House size: 1800 sq ft</span>
    <span class="comment"># Predicted price: $287,500</span></code></pre>
                </div>
            </article>

            <!-- Random Forest -->
            <article class="reference-card">
                <div class="reference-header collapsed" onclick="toggleReferenceContent(this)">
                    <h2>Random Forest</h2>
                    <span class="reference-category">Ensemble Learning</span>
                </div>
                <div class="reference-body collapsed">
                    <p>
                        Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate predictor. It addresses the overfitting problem of individual decision trees by averaging predictions from many trees, each trained on different subsets of data and features.
                    </p>

                    <h3>How Random Forest Works</h3>
                    <p>The algorithm builds multiple decision trees using two key randomization techniques:</p>
                    <ul>
                        <li><strong>Bootstrap Sampling (Bagging):</strong> Each tree is trained on a random sample of the training data, drawn with replacement</li>
                        <li><strong>Feature Randomness:</strong> At each split in each tree, only a random subset of features is considered</li>
                    </ul>

                    <h3>Mathematical Foundation</h3>
                    <p>For a Random Forest with <strong>n</strong> trees, the final prediction is:</p>
                    
                    <div class="math-equation">
                        <strong>Regression:</strong> ≈∑ = (1/n) √ó Œ£(i=1 to n) tree_i(x)
                    </div>
                    
                    <div class="math-equation">
                        <strong>Classification:</strong> ≈∑ = mode{tree_1(x), tree_2(x), ..., tree_n(x)}
                    </div>

                    <h3>Key Advantages</h3>
                    <ul>
                        <li><strong>Reduced Overfitting:</strong> Averaging multiple trees reduces variance</li>
                        <li><strong>Feature Importance:</strong> Provides built-in feature importance scores</li>
                        <li><strong>Handles Missing Values:</strong> Can work with incomplete data</li>
                        <li><strong>No Scaling Required:</strong> Tree-based methods are scale-invariant</li>
                        <li><strong>Handles Mixed Data Types:</strong> Works with both numerical and categorical features</li>
                    </ul>

                    <h3>Hyperparameters</h3>
                    <ul>
                        <li><strong>n_estimators:</strong> Number of trees in the forest (default: 100)</li>
                        <li><strong>max_features:</strong> Number of features to consider at each split (default: ‚àön_features for classification, n_features/3 for regression)</li>
                        <li><strong>max_depth:</strong> Maximum depth of trees (default: None)</li>
                        <li><strong>min_samples_split:</strong> Minimum samples required to split a node (default: 2)</li>
                        <li><strong>min_samples_leaf:</strong> Minimum samples required at a leaf node (default: 1)</li>
                    </ul>

                    <h3>Feature Importance</h3>
                    <p>Random Forest calculates feature importance by measuring how much each feature decreases impurity when used for splits across all trees:</p>
                    
                    <div class="math-equation">
                        <strong>Importance(feature_j) = Œ£(all trees) Œ£(all nodes using feature_j) (impurity_decrease √ó samples_fraction)</strong>
                    </div>

                    <h3>Out-of-Bag (OOB) Error</h3>
                    <p>
                        Since each tree is trained on a bootstrap sample (~63% of data), the remaining ~37% can be used as a validation set. This provides an unbiased estimate of model performance without needing a separate validation set.
                    </p>

                    <h3>Python Implementation Example</h3>
                    <pre class="code-block"><code><span class="keyword">from</span> <span class="module">sklearn.ensemble</span> <span class="keyword">import</span> <span class="class">RandomForestClassifier</span>, <span class="class">RandomForestRegressor</span>
<span class="keyword">from</span> <span class="module">sklearn.datasets</span> <span class="keyword">import</span> <span class="function">make_classification</span>, <span class="function">make_regression</span>
<span class="keyword">from</span> <span class="module">sklearn.model_selection</span> <span class="keyword">import</span> <span class="function">train_test_split</span>
<span class="keyword">from</span> <span class="module">sklearn.metrics</span> <span class="keyword">import</span> <span class="function">accuracy_score</span>, <span class="function">mean_squared_error</span>
<span class="keyword">import</span> <span class="module">numpy</span> <span class="keyword">as</span> <span class="variable">np</span>

<span class="comment"># Classification Example</span>
<span class="variable">X_class</span>, <span class="variable">y_class</span> <span class="operator">=</span> <span class="function">make_classification</span>(<span class="parameter">n_samples</span><span class="operator">=</span><span class="number">1000</span>, <span class="parameter">n_features</span><span class="operator">=</span><span class="number">20</span>, 
                                       <span class="parameter">n_informative</span><span class="operator">=</span><span class="number">10</span>, <span class="parameter">n_redundant</span><span class="operator">=</span><span class="number">5</span>, 
                                       <span class="parameter">random_state</span><span class="operator">=</span><span class="number">42</span>)
<span class="variable">X_train</span>, <span class="variable">X_test</span>, <span class="variable">y_train</span>, <span class="variable">y_test</span> <span class="operator">=</span> <span class="function">train_test_split</span>(<span class="variable">X_class</span>, <span class="variable">y_class</span>, 
                                                    <span class="parameter">test_size</span><span class="operator">=</span><span class="number">0.2</span>, <span class="parameter">random_state</span><span class="operator">=</span><span class="number">42</span>)

<span class="comment"># Create and train Random Forest classifier</span>
<span class="variable">rf_classifier</span> <span class="operator">=</span> <span class="class">RandomForestClassifier</span>(
    <span class="parameter">n_estimators</span><span class="operator">=</span><span class="number">100</span>,        <span class="comment"># Number of trees</span>
    <span class="parameter">max_features</span><span class="operator">=</span><span class="string">'sqrt'</span>,     <span class="comment"># ‚àön_features at each split</span>
    <span class="parameter">max_depth</span><span class="operator">=</span><span class="number">10</span>,            <span class="comment"># Maximum tree depth</span>
    <span class="parameter">min_samples_split</span><span class="operator">=</span><span class="number">5</span>,     <span class="comment"># Min samples to split</span>
    <span class="parameter">min_samples_leaf</span><span class="operator">=</span><span class="number">2</span>,      <span class="comment"># Min samples per leaf</span>
    <span class="parameter">oob_score</span><span class="operator">=</span><span class="boolean">True</span>,          <span class="comment"># Calculate OOB score</span>
    <span class="parameter">random_state</span><span class="operator">=</span><span class="number">42</span>
)

<span class="variable">rf_classifier</span>.<span class="method">fit</span>(<span class="variable">X_train</span>, <span class="variable">y_train</span>)
<span class="variable">y_pred_class</span> <span class="operator">=</span> <span class="variable">rf_classifier</span>.<span class="method">predict</span>(<span class="variable">X_test</span>)

<span class="function">print</span>(<span class="f-string">f"Classification Accuracy: {<span class="function">accuracy_score</span>(<span class="variable">y_test</span>, <span class="variable">y_pred_class</span>):<span class="format">.3f</span>}"</span>)
<span class="function">print</span>(<span class="f-string">f"OOB Score: {<span class="variable">rf_classifier</span>.<span class="attribute">oob_score_</span>:<span class="format">.3f</span>}"</span>)

<span class="comment"># Feature importance</span>
<span class="variable">feature_importance</span> <span class="operator">=</span> <span class="variable">rf_classifier</span>.<span class="attribute">feature_importances_</span>
<span class="function">print</span>(<span class="f-string">f"Top 3 most important features: {<span class="variable">np</span>.<span class="method">argsort</span>(<span class="variable">feature_importance</span>)[<span class="operator">-</span><span class="number">3</span>:]}"</span>)

<span class="comment"># Regression Example</span>
<span class="variable">X_reg</span>, <span class="variable">y_reg</span> <span class="operator">=</span> <span class="function">make_regression</span>(<span class="parameter">n_samples</span><span class="operator">=</span><span class="number">1000</span>, <span class="parameter">n_features</span><span class="operator">=</span><span class="number">10</span>, 
                               <span class="parameter">noise</span><span class="operator">=</span><span class="number">0.1</span>, <span class="parameter">random_state</span><span class="operator">=</span><span class="number">42</span>)
<span class="variable">X_train_reg</span>, <span class="variable">X_test_reg</span>, <span class="variable">y_train_reg</span>, <span class="variable">y_test_reg</span> <span class="operator">=</span> <span class="function">train_test_split</span>(
    <span class="variable">X_reg</span>, <span class="variable">y_reg</span>, <span class="parameter">test_size</span><span class="operator">=</span><span class="number">0.2</span>, <span class="parameter">random_state</span><span class="operator">=</span><span class="number">42</span>)

<span class="comment"># Create and train Random Forest regressor</span>
<span class="variable">rf_regressor</span> <span class="operator">=</span> <span class="class">RandomForestRegressor</span>(
    <span class="parameter">n_estimators</span><span class="operator">=</span><span class="number">100</span>,
    <span class="parameter">max_features</span><span class="operator">=</span><span class="string">'sqrt'</span>,
    <span class="parameter">oob_score</span><span class="operator">=</span><span class="boolean">True</span>,
    <span class="parameter">random_state</span><span class="operator">=</span><span class="number">42</span>
)

<span class="variable">rf_regressor</span>.<span class="method">fit</span>(<span class="variable">X_train_reg</span>, <span class="variable">y_train_reg</span>)
<span class="variable">y_pred_reg</span> <span class="operator">=</span> <span class="variable">rf_regressor</span>.<span class="method">predict</span>(<span class="variable">X_test_reg</span>)

<span class="function">print</span>(<span class="f-string">f"Regression MSE: {<span class="function">mean_squared_error</span>(<span class="variable">y_test_reg</span>, <span class="variable">y_pred_reg</span>):<span class="format">.3f</span>}"</span>)
<span class="function">print</span>(<span class="f-string">f"Regression R¬≤: {<span class="variable">rf_regressor</span>.<span class="method">score</span>(<span class="variable">X_test_reg</span>, <span class="variable">y_test_reg</span>):<span class="format">.3f</span>}"</span>)

<span class="comment"># Example output:</span>
<span class="comment"># Classification Accuracy: 0.925</span>
<span class="comment"># OOB Score: 0.918</span>
<span class="comment"># Top 3 most important features: [7 1 3]</span>
<span class="comment"># Regression MSE: 0.085</span>
<span class="comment"># Regression R¬≤: 0.892</span></code></pre>

                    <h3>When to Use Random Forest</h3>
                    <div class="comparison-table">
                        <div class="comparison-row">
                            <div class="comparison-header">‚úÖ Use Random Forest When:</div>
                            <div class="comparison-header">‚ùå Consider Alternatives When:</div>
                        </div>
                        <div class="comparison-row">
                            <div class="comparison-item">
                                <ul>
                                    <li>You have mixed data types (numerical + categorical)</li>
                                    <li>You need feature importance insights</li>
                                    <li>Data has missing values</li>
                                    <li>You want good performance with minimal tuning</li>
                                    <li>Interpretability is somewhat important</li>
                                    <li>You have medium-sized datasets (&lt;100k samples)</li>
                                </ul>
                            </div>
                            <div class="comparison-item">
                                <ul>
                                    <li>You need maximum interpretability (use single decision tree)</li>
                                    <li>You have very large datasets (consider XGBoost/LightGBM)</li>
                                    <li>You need probability calibration (use logistic regression)</li>
                                    <li>You have high-dimensional sparse data (use linear models)</li>
                                    <li>Training time is critical (use simpler models)</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <h3>Tuning Tips</h3>
                    <ul>
                        <li><strong>Start with defaults:</strong> Random Forest works well out-of-the-box</li>
                        <li><strong>Increase n_estimators:</strong> More trees usually help (with diminishing returns after ~100-500)</li>
                        <li><strong>Tune max_features:</strong> Try 'sqrt', 'log2', or specific numbers</li>
                        <li><strong>Control overfitting:</strong> Limit max_depth, increase min_samples_split/leaf</li>
                        <li><strong>Use OOB score:</strong> Monitor out-of-bag error instead of validation set</li>
                    </ul>
                </div>
            </article>

<section class="references-footer">
    <div class="container">
        <div class="footer-content">
            <h2>Additional Resources</h2>
            <div class="resources-grid">
                <div class="resource-item">
                    <h3>üìö Recommended Books</h3>
                    <ul>
                        <li>"Hands-On Machine Learning" by Aur√©lien G√©ron</li>
                        <li>"The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman</li>
                        <li>"Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
                    </ul>
                </div>
                <div class="resource-item">
                    <h3>üîß Essential Libraries</h3>
                    <ul>
                        <li><strong>Scikit-learn:</strong> General-purpose ML library</li>
                        <li><strong>TensorFlow/PyTorch:</strong> Deep learning frameworks</li>
                        <li><strong>Pandas:</strong> Data manipulation and analysis</li>
                        <li><strong>NumPy:</strong> Numerical computing</li>
                    </ul>
                </div>
                <div class="resource-item">
                    <h3>üåê Online Courses</h3>
                    <ul>
                        <li>Andrew Ng's Machine Learning Course (Coursera)</li>
                        <li>Fast.ai Practical Deep Learning</li>
                        <li>CS231n: Convolutional Neural Networks (Stanford)</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>

<script>
// Sample house data.
const houseData = [{size: 1000, price: 150000}, {size: 1500, price: 225000}, {size: 2000, price: 300000}];
// Utility functions for drawing.
function clearCanvas(canvas) { const ctx = canvas.getContext('2d'); ctx.clearRect(0, 0, canvas.width, canvas.height); }
function drawAxes(ctx, canvas) { const margin = 50; ctx.strokeStyle = '#374151'; ctx.lineWidth = 2; ctx.beginPath(); ctx.moveTo(margin, canvas.height - margin); ctx.lineTo(canvas.width - margin, canvas.height - margin); ctx.stroke(); ctx.beginPath(); ctx.moveTo(margin, margin); ctx.lineTo(margin, canvas.height - margin); ctx.stroke(); ctx.fillStyle = '#374151'; ctx.font = '12px Inter'; ctx.fillText('House Size (sq ft)', canvas.width/2 - 50, canvas.height - 10); ctx.save(); ctx.translate(15, canvas.height/2 + 30); ctx.rotate(-Math.PI/2); ctx.fillText('Price ($)', 0, 0); ctx.restore(); }
function scalePoint(value, min, max, pixelMin, pixelMax) { return pixelMin + (value - min) * (pixelMax - pixelMin) / (max - min); }
// Show data points and fitted line.
function showDataFit() { const canvas = document.getElementById('dataFitCanvas'); const ctx = canvas.getContext('2d'); clearCanvas(canvas); const margin = 50; const minSize = 800, maxSize = 2200; const minPrice = 100000, maxPrice = 350000; drawAxes(ctx, canvas); ctx.fillStyle = '#3b82f6'; houseData.forEach(point => { const x = scalePoint(point.size, minSize, maxSize, margin, canvas.width - margin); const y = scalePoint(point.price, minPrice, maxPrice, canvas.height - margin, margin); ctx.beginPath(); ctx.arc(x, y, 8, 0, 2 * Math.PI); ctx.fill(); ctx.fillStyle = '#1f2937'; ctx.font = '10px Inter'; ctx.fillText(`(${point.size}, $${point.price/1000}k)`, x + 10, y - 10); ctx.fillStyle = '#3b82f6'; }); ctx.strokeStyle = '#ef4444'; ctx.lineWidth = 3; ctx.beginPath(); const x1 = scalePoint(minSize, minSize, maxSize, margin, canvas.width - margin); const y1 = scalePoint(150 * minSize, minPrice, maxPrice, canvas.height - margin, margin); const x2 = scalePoint(maxSize, minSize, maxSize, margin, canvas.width - margin); const y2 = scalePoint(150 * maxSize, minPrice, maxPrice, canvas.height - margin, margin); ctx.moveTo(x1, y1); ctx.lineTo(x2, y2); ctx.stroke(); ctx.fillStyle = '#ef4444'; ctx.font = '14px Inter'; ctx.fillText('≈∑ = 150x + 0', canvas.width - 120, margin + 20); }
// Show cost reduction over iterations.
function showCostReduction() { const canvas = document.getElementById('costCanvas'); const ctx = canvas.getContext('2d'); clearCanvas(canvas); const margin = 50; const iterations = Array.from({length: 50}, (_, i) => i); const costs = iterations.map(i => Math.exp(-i/10) * 27180000000 + Math.random() * 1000000000); drawAxes(ctx, canvas); ctx.strokeStyle = '#8b5cf6'; ctx.lineWidth = 2; ctx.beginPath(); const maxCost = Math.max(...costs); costs.forEach((cost, i) => { const x = scalePoint(i, 0, iterations.length - 1, margin, canvas.width - margin); const y = scalePoint(cost, 0, maxCost, canvas.height - margin, margin); if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y); }); ctx.stroke(); ctx.fillStyle = '#374151'; ctx.font = '12px Inter'; ctx.fillText('Iterations', canvas.width/2 - 30, canvas.height - 10); ctx.save(); ctx.translate(15, canvas.height/2 + 20); ctx.rotate(-Math.PI/2); ctx.fillText('Cost (MSE)', 0, 0); ctx.restore(); ctx.fillStyle = '#8b5cf6'; ctx.font = '14px Inter'; ctx.fillText('Cost decreases as model learns', margin, margin - 10); }
// Animate gradient descent.
function animateGradientDescent() { const canvas = document.getElementById('costCanvas'); const ctx = canvas.getContext('2d'); let frame = 0; function animate() { clearCanvas(canvas); drawAxes(ctx, canvas); const margin = 50; const iterations = Math.min(frame, 50); const costs = Array.from({length: iterations + 1}, (_, i) => Math.exp(-i/10) * 27180000000 + Math.random() * 500000000); ctx.strokeStyle = '#8b5cf6'; ctx.lineWidth = 3; ctx.beginPath(); const maxCost = 30000000000; costs.forEach((cost, i) => { const x = scalePoint(i, 0, 50, margin, canvas.width - margin); const y = scalePoint(cost, 0, maxCost, canvas.height - margin, margin); if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y); }); ctx.stroke(); if (costs.length > 0) { const currentCost = costs[costs.length - 1]; const x = scalePoint(iterations, 0, 50, margin, canvas.width - margin); const y = scalePoint(currentCost, 0, maxCost, canvas.height - margin, margin); ctx.fillStyle = '#ef4444'; ctx.beginPath(); ctx.arc(x, y, 6, 0, 2 * Math.PI); ctx.fill(); } ctx.fillStyle = '#374151'; ctx.font = '12px Inter'; ctx.fillText(`Iteration: ${iterations}`, margin, margin - 10); frame++; if (frame <= 50) { setTimeout(() => requestAnimationFrame(animate), 100); } } animate(); }
// Interactive parameter explorer.
function updateLine() { const canvas = document.getElementById('interactiveCanvas'); const ctx = canvas.getContext('2d'); clearCanvas(canvas); const margin = 50; const minSize = 800, maxSize = 2200; const minPrice = 50000, maxPrice = 400000; drawAxes(ctx, canvas); const slope = parseFloat(document.getElementById('slopeSlider').value); const intercept = parseFloat(document.getElementById('interceptSlider').value); document.getElementById('slopeValue').textContent = slope; document.getElementById('interceptValue').textContent = intercept.toLocaleString(); ctx.fillStyle = '#3b82f6'; let totalCost = 0; houseData.forEach(point => { const x = scalePoint(point.size, minSize, maxSize, margin, canvas.width - margin); const y = scalePoint(point.price, minPrice, maxPrice, canvas.height - margin, margin); ctx.beginPath(); ctx.arc(x, y, 8, 0, 2 * Math.PI); ctx.fill(); const prediction = slope * point.size + intercept; const error = point.price - prediction; totalCost += error * error; const predY = scalePoint(prediction, minPrice, maxPrice, canvas.height - margin, margin); ctx.strokeStyle = '#fbbf24'; ctx.lineWidth = 1; ctx.setLineDash([5, 5]); ctx.beginPath(); ctx.moveTo(x, y); ctx.lineTo(x, predY); ctx.stroke(); ctx.setLineDash([]); }); ctx.strokeStyle = '#ef4444'; ctx.lineWidth = 3; ctx.beginPath(); const x1 = scalePoint(minSize, minSize, maxSize, margin, canvas.width - margin); const y1 = scalePoint(slope * minSize + intercept, minPrice, maxPrice, canvas.height - margin, margin); const x2 = scalePoint(maxSize, minSize, maxSize, margin, canvas.width - margin); const y2 = scalePoint(slope * maxSize + intercept, minPrice, maxPrice, canvas.height - margin, margin); ctx.moveTo(x1, y1); ctx.lineTo(x2, y2); ctx.stroke(); const mse = totalCost / houseData.length; document.getElementById('currentCost').textContent = mse.toLocaleString(undefined, {maximumFractionDigits: 0}); ctx.fillStyle = '#ef4444'; ctx.font = '14px Inter'; ctx.fillText(`≈∑ = ${slope}x + ${intercept}`, canvas.width - 150, margin + 20); }

// Logistic regression demo data and helpers.
const logisticData = [
    {x1: 1.0, x2: 1.0, y: 0}, {x1: 1.8, x2: 0.8, y: 0}, {x1: 1.2, x2: 1.5, y: 0}, {x1: 2.0, x2: 1.0, y: 0},
    {x1: 2.6, x2: 2.7, y: 1}, {x1: 3.0, x2: 3.2, y: 1}, {x1: 2.8, x2: 2.4, y: 1}, {x1: 3.2, x2: 2.8, y: 1}
];
function logit(p) { return Math.log(p / (1 - p)); }
function drawAxes2D(ctx, canvas, xLabel, yLabel) { const margin = 40; ctx.strokeStyle = '#374151'; ctx.lineWidth = 2; ctx.beginPath(); ctx.moveTo(margin, canvas.height - margin); ctx.lineTo(canvas.width - margin, canvas.height - margin); ctx.stroke(); ctx.beginPath(); ctx.moveTo(margin, margin); ctx.lineTo(margin, canvas.height - margin); ctx.stroke(); ctx.fillStyle = '#374151'; ctx.font = '12px Inter'; ctx.fillText(xLabel, canvas.width/2 - 10, canvas.height - 8); ctx.save(); ctx.translate(12, canvas.height/2 + 10); ctx.rotate(-Math.PI/2); ctx.fillText(yLabel, 0, 0); ctx.restore(); }
function updateLogistic() { const canvas = document.getElementById('logisticCanvas'); if (!canvas) { return; } const ctx = canvas.getContext('2d'); const margin = 40; const xMin = 0, xMax = 4, yMin = 0, yMax = 4; const sx = (x) => margin + (x - xMin) * (canvas.width - 2 * margin) / (xMax - xMin); const sy = (y) => canvas.height - margin - (y - yMin) * (canvas.height - 2 * margin) / (yMax - yMin); const t0 = parseFloat(document.getElementById('theta0Slider').value); const t1 = parseFloat(document.getElementById('theta1Slider').value); const t2 = parseFloat(document.getElementById('theta2Slider').value); const thresh = parseFloat(document.getElementById('threshSlider').value); document.getElementById('theta0Value').textContent = t0.toFixed(1); document.getElementById('theta1Value').textContent = t1.toFixed(1); document.getElementById('theta2Value').textContent = t2.toFixed(1); document.getElementById('threshValue').textContent = thresh.toFixed(2); clearCanvas(canvas); drawAxes2D(ctx, canvas, 'x‚ÇÅ', 'x‚ÇÇ'); logisticData.forEach(p => { ctx.beginPath(); ctx.arc(sx(p.x1), sy(p.x2), 6, 0, Math.PI * 2); ctx.fillStyle = p.y === 1 ? '#10b981' : '#ef4444'; ctx.fill(); }); const L = logit(thresh); ctx.strokeStyle = '#3b82f6'; ctx.lineWidth = 2; if (Math.abs(t2) > 1e-6) { const xA = xMin, xB = xMax; const yA = (L - t0 - t1 * xA) / t2; const yB = (L - t0 - t1 * xB) / t2; ctx.beginPath(); ctx.moveTo(sx(xA), sy(yA)); ctx.lineTo(sx(xB), sy(yB)); ctx.stroke(); } else if (Math.abs(t1) > 1e-6) { const xV = (L - t0) / t1; ctx.beginPath(); ctx.moveTo(sx(xV), sy(yMin)); ctx.lineTo(sx(xV), sy(yMax)); ctx.stroke(); } ctx.fillStyle = '#3b82f6'; ctx.font = '12px Inter'; ctx.fillText('Decision boundary', canvas.width - 150, margin + 10); }
// Toggle reference content visibility.
function toggleReferenceContent(header) { const referenceBody = header.nextElementSibling; const isCollapsed = referenceBody.classList.contains('collapsed'); if (isCollapsed) { header.classList.remove('collapsed'); referenceBody.classList.remove('collapsed'); } else { header.classList.add('collapsed'); referenceBody.classList.add('collapsed'); } }
// Initialize graphs when page loads.
document.addEventListener('DOMContentLoaded', function() { setTimeout(showDataFit, 100); setTimeout(updateLine, 200); setTimeout(updateLogistic, 300); });
</script>

{% endblock %}